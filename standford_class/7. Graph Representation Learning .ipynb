{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ideas: maximum-likelihood:\n",
    "0) the maximum likelihood estimation was converted into a negative sampling. Essentially the paper, explains why maximizing the true max likelihood can be well approximated by negative sampling. Notice two differences between max-likelihood and negative sampling loss functions \n",
    "    * the sum log and log of sum\n",
    "    * exponents are replaced by sigmoid \n",
    "\n",
    "## ideas: link prediction via classification \n",
    "1) nodes are represente not by their index, but by something like their name. That makes a node and its neighbors more like a context. So when we see a node in another area even if it does not have a lot of neighbors, the embedding for it is generated from nodes for which we have a lot of embeddings\n",
    "\n",
    "2) once you have word2vec type node2vec embedding, you can grab 2 node embeddings, and from it generate one embeddings. Then apply a classifier, that says if there is a connection between them or not. \n",
    "    * I think you should apply these to nodes for which we have some data. Otherwise, the model will tend to say there is no link.\n",
    "    * you will need negative sampling as most nodes are not connected. This model can be easily trained based on the existing graphs and true connections, and also validated. \n",
    "\n",
    "## ideas 2: link prediction via knowledge graph:\n",
    "1) here we can not only predict the link, but we can also predict the type of link\n",
    "    * we can use this to impute missing manufacturer and also children separately.\n",
    "    * once again i feel we need to do this for a well connected graph and then extend it to the missing parts. \n",
    "    * this will allow us to find the best children for a parent as well as best manufacturer\n",
    "    \n",
    "    \n",
    "## ideas 3: graph embedding ?\n",
    "1) maybe can help us detect if a line number graph is very different from another (perhaps one is missing alot of data ? but how do you ensure the embeddings from multiple graphs are aligned, you have to train them at the same time ?!?!\n",
    "2) can this be applied to any sub-graph ? or series of nodes ? and then compare them  against another set of nodes ? can we find similar subgraphs (assemblies ?)\n",
    "3) in a search engine application, can we use this for document similarity ?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision_class",
   "language": "python",
   "name": "comp_vis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
